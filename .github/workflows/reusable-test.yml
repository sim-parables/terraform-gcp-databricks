name: '[Reusable] Databricks Workspace Job Test'

on:
  workflow_call:
    inputs:
      ACT_MODE:
        description: |
          For running Github Action Workflows locally with Nektos act
        required: true
        type: boolean
      DATABRICKS_PROFILE:
        description: |
          Databricks CLI configuration profile name for Databricks Accounts configuration/credentials
        required: false
        type: string
        default: GCP_WORKSPACE
      DATABRICKS_ADMINISTRATOR:
        description: |
          Databricks Accounts and Workspace administrator email
        required: false
        type: string
        default: jason.grein@gmail.com
      DATABRICKS_ASSET_BUNDLE_URL:
        description: |
          Databricks Asset Bundle Template Git URL
        required: false
        type: string
        default: https://github.com/sim-parables/databricks-xcloud-asset-bundle-template
      DATABRICKS_ASSET_BUNDLE_PROJECT_NAME:
        description: |
          Databricks Asset Bundle project name configured with DAB template
        required: false
        type: string
        default: sim_parables_dab_example
      working_directory:
        description: |
          Working directory containing Terraform test scripts.
        required: false
        type: string
        default: "./test"
    
    secrets:
      GCP_DATABRICKS_WORKSPACE_HOST:
        description: |
          GCP Databricks Workspace Host URL
        required: true
      GCP_DATABRICKS_WORKSPACE_TOKEN:
        description: |
          GCP Databricks Workspace Service Principal Token
        required: true
      TF_API_TOKEN:
        description: |
          Terraform.io Access Token
        required: true

#Special permissions required for OIDC authentication
permissions:
  id-token: write
  contents: read

jobs:
  terraform-output:
    name: Terraform Output
    runs-on: ubuntu-latest
    
    defaults:
        run:
          working-directory: ${{ inputs.working_directory }}
    
    outputs:
      DATABRICKS_CLUSTER_ID: ${{ steps.tf-raw.outputs.DATABRICKS_CLUSTER_ID }}
      SERVICE_ACCOUNT_KEY_NAME: ${{ steps.tf-raw.outputs.SERVICE_ACCOUNT_KEY_NAME }}
      SERVICE_ACCOUNT_KEY_SECRET: ${{ steps.tf-raw.outputs.SERVICE_ACCOUNT_KEY_SECRET }}
      OUTPUT_DIR: ${{ steps.tf-raw.outputs.OUTPUT_DIR }}
      OUTPUT_TABLE: ${{ steps.tf-raw.outputs.OUTPUT_TABLE }}
      EXAMPLE_HOLDING_FILE_PATH: ${{ steps.tf-raw.outputs.EXAMPLE_HOLDING_FILE_PATH }}
      EXAMPLE_WEATHER_FILE_PATH: ${{ steps.tf-raw.outputs.EXAMPLE_WEATHER_FILE_PATH }}

    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      uses: actions/checkout@v4
    
    # Install the latest version of the Terraform CLI
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
        terraform_wrapper: false
    
    - name: terraform init
      run: terraform init

    - name: terraform raw output
      id: tf-raw
      run: |
        echo "DATABRICKS_CLUSTER_ID=$(terraform output -json databricks_cluster_ids | jq -r '.[0]')
        SERVICE_ACCOUNT_KEY_NAME=$(terraform output -raw google_secret_client_id_name)
        SERVICE_ACCOUNT_KEY_SECRET=$(terraform output -raw google_secret_client_secret_name)
        OUTPUT_DIR=$(terraform output -raw databricks_external_location_url)
        OUTPUT_TABLE=$(terraform output -json databricks_unity_catalog_table_paths | jq -r '.[0]')
        EXAMPLE_HOLDING_FILE_PATH=$(terraform output -raw databricks_example_holdings_data_path)
        EXAMPLE_WEATHER_FILE_PATH=$(terraform output -raw databricks_example_weather_data_path)" >> $GITHUB_OUTPUT
  
  databricks-gcp-deploy:
    name: Databricks GCP DAB Deploy
    needs: [ terraform-output ]
    runs-on: ubuntu-latest
    
    defaults:
      run:
        working-directory: "${{ inputs.working_directory }}"
    
    steps:
      - uses: actions/checkout@v4
        
      - name: Setup Databricks CLI
        uses: databricks/setup-cli@main
      
      - name: Databricks Accounts Configuration
        run: |
          echo "" > /root/.databrickscfg
          echo "[${{ inputs.DATABRICKS_PROFILE }}]
          host                = ${{ secrets.GCP_DATABRICKS_WORKSPACE_HOST }}
          token               = ${{ secrets.GCP_DATABRICKS_WORKSPACE_TOKEN }}
          jobs-api-version    = 2.1" > /root/.databrickscfg
      
      # Initialize Databricks Asset Bundle
      # DO NOT user hyphens "-" in project name - will cause package namespace to be broken in Python
      - name: Build Databricks Asset Bundle Config File
        run: |
          echo "{
            \"project_name\": \"${{ inputs.DATABRICKS_ASSET_BUNDLE_PROJECT_NAME }}\",
            \"distribution_list\": \"${{ inputs.DATABRICKS_ADMINISTRATOR }}\",
            \"databricks_cli_profile\": \"${{ inputs.DATABRICKS_PROFILE }}\",
            \"databricks_cloud_provider\": \"GCP\",
            \"databricks_service_account_key_name\": \"${{ needs.terraform-output.outputs.SERVICE_ACCOUNT_KEY_NAME }}\",
            \"databricks_service_account_key_secret\": \"${{ needs.terraform-output.outputs.SERVICE_ACCOUNT_KEY_SECRET }}\"
          }" > databricks_dab_template_config
      
      - name: Build & Deploy Databricks Asset Bundle
        run: |
          export DATABRICKS_CONFIG_FILE=/root/.databrickscfg && \
          databricks bundle init ${{ inputs.DATABRICKS_ASSET_BUNDLE_URL }} \
            --output-dir=dab_solution \
            --config-file=databricks_dab_template_config \
            --profile=${{ inputs.DATABRICKS_PROFILE }}
          
          cd dab_solution
          databricks bundle deploy \
          --var="databricks_cluster_id=${{ needs.terraform-output.outputs.DATABRICKS_CLUSTER_ID }},csv_holdings_path=${{ needs.terraform-output.outputs.EXAMPLE_HOLDING_FILE_PATH }},csv_weather_path=${{ needs.terraform-output.outputs.EXAMPLE_WEATHER_FILE_PATH }},output_path=${{ needs.terraform-output.outputs.OUTPUT_DIR }},output_table=${{ needs.terraform-output.outputs.OUTPUT_TABLE }}" \
          --profile=${{ inputs.DATABRICKS_PROFILE }}
        
          databricks bundle run ${{ inputs.DATABRICKS_ASSET_BUNDLE_PROJECT_NAME }}_example_output \
          --profile=${{ inputs.DATABRICKS_PROFILE }} \
          --var="databricks_cluster_id=${{ needs.terraform-output.outputs.DATABRICKS_CLUSTER_ID }},csv_holdings_path=${{ needs.terraform-output.outputs.EXAMPLE_HOLDING_FILE_PATH }},csv_weather_path=${{ needs.terraform-output.outputs.EXAMPLE_WEATHER_FILE_PATH }},output_path=${{ needs.terraform-output.outputs.OUTPUT_DIR }},output_table=${{ needs.terraform-output.outputs.OUTPUT_TABLE }}"

          databricks bundle run ${{ inputs.DATABRICKS_ASSET_BUNDLE_PROJECT_NAME }}_example_output_uc \
          --profile=${{ inputs.DATABRICKS_PROFILE }} \
          --var="databricks_cluster_id=${{ needs.terraform-output.outputs.DATABRICKS_CLUSTER_ID }},csv_holdings_path=${{ needs.terraform-output.outputs.EXAMPLE_HOLDING_FILE_PATH }},csv_weather_path=${{ needs.terraform-output.outputs.EXAMPLE_WEATHER_FILE_PATH }},output_path=${{ needs.terraform-output.outputs.OUTPUT_DIR }},output_table=${{ needs.terraform-output.outputs.OUTPUT_TABLE }}"
